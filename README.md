# Mobility Expreiments
## YOLOv11 train

This repository provides a full evaluation pipeline for YOLOv11 model predictions using custom ground truth and prediction files.

## Custom dataset
```
â”œâ”€â”€ test       # Flat format ground truth (from YOLO .txts)â”œ
â”‚    â”‚
â”‚    â”œâ”€â”€
â”œâ”€â”€ train      # YOLOv11 inference output (COCO format)
â”‚    â”‚
â”‚
â”œâ”€â”€ valid      # ../train
â”‚    â”‚
â”‚
â”œâ”€â”€ data.yaml  # YOLOv11 yaml file

```

## ğŸš€ Usage
### 1. Install Requirements
```bash
pip install -r requirements.txt
```

If `metrics.py` uses torch:
```bash
pip install torch torchvision matplotlib
```

### 2. Prepare Data
- Convert YOLO `.txt` GT files to `gt_annotations.json`
- Save `predictions.json` from YOLOv11
- Update paths in `evaluate.py`

### 3. Run Evaluation
```bash
python evaluate.py
```

### 4. Output
- Console: Precision, Recall, mAP@0.5, mAP@0.5:0.95, F1
- Saved files: `metrics_out/*.png` PR/F1/Confidence curves

---

## ğŸ“Š Evaluation Metrics
- `Precision`, `Recall`, `F1 Score`
- `mAP@0.5`, `mAP@0.5:0.95`
- Per-class AP (from DetMetrics)

## ğŸ” Access Control
This repo is **Private**. Only invited collaborators can access.  
To share:
- Go to `Settings` â†’ `Collaborators`
- Invite GitHub accounts manually

---

## ğŸ™‹â€â™‚ï¸ Author & License
- Author: `your_name`
- License: MIT or custom (your choice)

---

## âœ… To Do
- [ ] Class-wise AP to CSV export
- [ ] COCO Eval integration (pycocotools)
- [ ] Jupyter version for visual debugging


